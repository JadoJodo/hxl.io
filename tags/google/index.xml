<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Google on HXL</title><link>https://hxl.io/tags/google/</link><description>Recent content in Google on HXL</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>j@hxl.io (Jordan Finnigan)</managingEditor><webMaster>j@hxl.io (Jordan Finnigan)</webMaster><copyright>Copy, _right?_ :thinking_face:</copyright><lastBuildDate>Thu, 15 Feb 2024 19:42:49 +0000</lastBuildDate><atom:link href="https://hxl.io/tags/google/index.xml" rel="self" type="application/rss+xml"/><item><title>Orange House Cat</title><link>https://hxl.io/posts/2024/02/orange-house-cat/</link><pubDate>Thu, 15 Feb 2024 19:42:49 +0000</pubDate><author>j@hxl.io (Jordan Finnigan)</author><guid>https://hxl.io/posts/2024/02/orange-house-cat/</guid><description>There&amp;rsquo;s a weird edge case to using Large Language Models (LLMs) trained by humans that I&amp;rsquo;ve seen come up in Google&amp;rsquo;s reCAPTCHA: Sometimes it identifies an object that resembles the one they think it is, but it&amp;rsquo;s not actually the one they think it is.</description></item></channel></rss>